\documentclass[a4paper,final,ngerman,english]{article} 
\usepackage[a4paper,final,pdfdisplaydoctitle,pdfauthor={My name},pdftitle={Title of my paper}]{hyperref}
\usepackage{amsmath,amsthm,amssymb,amsfonts,pgf,algorithm,algorithmic,array,multirow,fixltx2e,colortbl,tabularx,rotating,listings,stmaryrd,ifsym,marvosym}
\usepackage[ngerman]{babel} %New german orthography
\usepackage[T1]{fontenc} 
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{lmodern} 
\pdfcompresslevel=9
%
\begin{document}
\lstset{postbreak=\space,breakindent=5pt,breaklines,basicstyle=\scriptsize,frame=trbl,label=DescriptiveLabel} %Set defaults for listings
\selectlanguage{english} %probably not necessary, just to be on the safe side
\pagestyle{headings}
%
\title{Plasma scaling study}
\author{Simon Haendeler, Severin Staudinger}
\institute{Fakultät für Informatik, Universität Wien}
\maketitle
%
\begin{abstract}
	We are examining the scaling of the PLASMA library in case of the Cholesky Decomposition.
	We compare the runtime when using a different amount of cores and compare this to the runtime
	of LAPACK\@.
\end{abstract}
\section{Introduction}
In this paper we demonstrate how a driver function of PLASMA \cite{plasma01} scales.
Therefor we decided to show this by the cholesky factorization, which is an important decomposition of symmetric positive definite matrices in the field of scientific computing.
To achieve this we implemented PLASMA's \textit{DPOTRF} routine and measured the processing time of its execution on two, four, eight, sixteen, 32 and 48 cores. For time measuring we used the library PAPI [2].\\
All measurements were done on a system with an x86\_64 architecture, 48 CPUs, one thread per core, AMD Opteron(tm) Processor 6174 with a theoretical peak performance of 2200.00 MHz. Furthermore there are 8 NUMA nodes, which combine six cores per each node.
In the end we compared PLASMA's \textit{DPOTRF} runtimes to the sequential cholesky decomposition implemented in LAPACK [3] and calculated absolute and relative speedups.

\section{Implementation}
In order to build PLASMA we installed OpenBLAS [4] and LAPACK [3] from the latest stable releases.
Since on our system is just one thread per core we built OpenBLAS single-threaded with \textit{MAKE USE\_THREAD=0}.
To guarantee portability we decided to make a bash-script, which install every necessary library fully automatically and also auto-generate a Makefile, which compiles our \textit{main.c} programm.
This is very useful, because anybody who is interested in this PLASMA scaling study can simply run this script with \textit{bash plasma\_lapack\_blas.sh} .
After small adjustments of the generated template\_Makefile the user can compile the programm with \textit{make}.
\\
To verify a correct execution of the test programm we used valgrind.




\section{Evaluation}
Because the measurement was done on a shared system
we could not guarantee that our program was the only one running.
Measurement errors through context switches and alike are possible.
Each datapoint represents 5 measurements that were averaged so that small measurement errors cancel out.
Systematic error could still be present i.e.\ long running processes which affect all measurements
are still possible. Although we tried to control for those manually via htop and similar tools, we cannot exclude
those with 100\% certainty.

In figure~%\ref{fig:runtime}
we see the runtime of the LAPACK version and of the PLASMA version with different amount of cores/threads used.
As expected the runtime mostly increases like \(x^3\).
The single threaded PLASMA version is slightly slower than the sequential LAPACK version.
The other PLASMA versions are faster the more cores there are, at least for $n$ large enough.
The versions for 8 and 16 threads are very close together. 32 and 48 threads are also close, although
we expect that this gap should be relativly narrow.
There are bumps in the graph of the LAPACK version for \(n = 4100\) and \(n = 5100 \).

In figure~%\ref{fig:rel}
we see the relative speedup for each version.
For comparison there are black lines at 2, 4, 8, 16, 32, 48 which represent the theoretical possible speedups.
We can see that the parallel versions start slower than the sequential version.
Then for relatively low $n$ each version archieves a local maxima.
For a higher thread number the $n$ is higher when the maxima is reached. 

Only 2 and 4 threads nearly reaches its theoretical performance, the higher the amount of threads,
the worse the performance relatively to the theoretical performance.

The residuum is shown in figure~%\ref{fig:residuum}
. All PLASMA versions have the same residuum behavior, but LAPACK is always a bit better.  
There are certain thresholds after which the residuual gets worse (200, 500, 1000, 2000, 4000).
After these sudden increases the residuum gets better with higher $n$.

\section{Conclusions}
We can see clearly that PLASMA scales with the cores as expected.
Sadly for a high number of threads it does not reach its theoretical peak performance.

%  
\bibliographystyle{plain} %Switches are plain,unsrt,abbrv,alpha.
\begin{footnotesize}
  \bibliography{techreport} %Publications
\end{footnotesize}
\end{document}

